{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02-BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we master the preprocessing, let's make our first Bag Of Words (BOW)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will reuse our dataset of Coldplay songs to make a BOW."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, the first step is to import some libraries. So import *nltk* as well as all the libraries you will need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import NLTK and all the needed libraries\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load now the dataset in *coldplay.csv* using pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Artist                           Song  \\\n",
      "0    Coldplay                 Another's Arms   \n",
      "1    Coldplay                Bigger Stronger   \n",
      "2    Coldplay                       Daylight   \n",
      "3    Coldplay                       Everglow   \n",
      "4    Coldplay  Every Teardrop Is A Waterfall   \n",
      "..        ...                            ...   \n",
      "115  Coldplay           Hymn For The Weekend   \n",
      "116  Coldplay                    In My Place   \n",
      "117  Coldplay                            Ink   \n",
      "118  Coldplay              Ladder To The Sun   \n",
      "119  Coldplay                           Lost   \n",
      "\n",
      "                                                  Link  \\\n",
      "0              /c/coldplay/anothers+arms_21079526.html   \n",
      "1            /c/coldplay/bigger+stronger_20032648.html   \n",
      "2                   /c/coldplay/daylight_20032625.html   \n",
      "3                   /c/coldplay/everglow_21104546.html   \n",
      "4    /c/coldplay/every+teardrop+is+a+waterfall_2091...   \n",
      "..                                                 ...   \n",
      "115     /c/coldplay/hymn+for+the+weekend_21104544.html   \n",
      "116              /c/coldplay/in+my+place_20032629.html   \n",
      "117                      /c/coldplay/ink_21082518.html   \n",
      "118        /c/coldplay/ladder+to+the+sun_20232934.html   \n",
      "119                     /c/coldplay/lost_20743853.html   \n",
      "\n",
      "                                                Lyrics  \n",
      "0    Late night watching tv  \\nUsed to be you here ...  \n",
      "1    I want to be bigger stronger drive a faster ca...  \n",
      "2    To my surprise, and my delight  \\nI saw sunris...  \n",
      "3    Oh, they say people come  \\nThey say people go...  \n",
      "4    I turn the music up, I got my records on  \\nI ...  \n",
      "..                                                 ...  \n",
      "115  Oh, angel sent from up above  \\nYou know you m...  \n",
      "116  In my place, in my place  \\nWere lines that I ...  \n",
      "117  Got a tattoo that says \"2gether thru life\"  \\n...  \n",
      "118  From the very start  \\nIt came apart  \\nIt bro...  \n",
      "119  Just because I'm losing  \\nDoesn't mean I'm lo...  \n",
      "\n",
      "[120 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Load the dataset in coldplay.csv\n",
    "data = pd.read_csv('coldplay.csv')\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You already know this dataset, but you can check it again if you want to refresh your memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120 entries, 0 to 119\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   Artist  120 non-null    object\n",
      " 1   Song    120 non-null    object\n",
      " 2   Link    120 non-null    object\n",
      " 3   Lyrics  120 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 3.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# TODO: Explore the data\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now using the *CountVectorizer* of scikit-learn, make a BOW of all the lyrics of Coldplay, and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(120, 1569)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 1 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Compute a BOW\n",
    "countVectorizer = CountVectorizer(max_features=2000, stop_words='english')\n",
    "BOW = countVectorizer.fit_transform(df.Lyrics).toarray()\n",
    "\n",
    "print(BOW.shape)\n",
    "print(BOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the BOW matrix, we would like to have a new dataframe having the BOW for each song, and as columns the corresponding words (just as we did in the lecture at the end).\n",
    "\n",
    "So that at the end we would end up with a dataframe containing something like the following (120 raws for 120 songs, and as many columns as words):\n",
    "\n",
    "| | ah | adventure | ... | yeah \n",
    "|---|---|---|---|---| \n",
    "| 0 | 0 | 1 | ... | 4 |\n",
    "| 1 | 8 | 0 | ... | 2 |\n",
    "|...|...|...|...|...|\n",
    "| 119 | 5 | 0 | ... | 8 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10' '2000' '2gether' ... 'yesterday' 'young' 'yuletide']\n",
      "     10  2000  2gether  76543  aaaaaah  aaaaah  aaaah  achin  adventure  \\\n",
      "0     0     0        0      0        0       0      0      0          0   \n",
      "1     0     0        0      0        0       0      0      0          0   \n",
      "2     0     0        0      0        0       0      0      0          0   \n",
      "3     0     0        0      0        0       0      0      0          0   \n",
      "4     0     0        0      0        0       0      0      0          0   \n",
      "..   ..   ...      ...    ...      ...     ...    ...    ...        ...   \n",
      "115   0     0        0      0        0       0      0      0          0   \n",
      "116   0     0        0      0        0       0      0      0          0   \n",
      "117   0     0        1      0        0       0      0      0          0   \n",
      "118   0     0        0      0        0       0      0      0          0   \n",
      "119   0     0        0      0        0       0      0      0          0   \n",
      "\n",
      "     advice  ...  x2  x7  ya  yeah  years  yellow  yes  yesterday  young  \\\n",
      "0         0  ...   0   0   0     0      0       0    0          0      0   \n",
      "1         0  ...   0   0   0     0      0       0    0          0      0   \n",
      "2         0  ...   0   0   0     2      0       0    0          0      0   \n",
      "3         0  ...   0   0   0     2      0       0    0          0      0   \n",
      "4         0  ...   0   0   0     0      0       0    0          0      0   \n",
      "..      ...  ...  ..  ..  ..   ...    ...     ...  ...        ...    ...   \n",
      "115       0  ...   0   0   0     0      0       0    0          0      0   \n",
      "116       0  ...   0   0   0    11      0       0    0          0      0   \n",
      "117       0  ...   0   0   0     3      0       0    0          0      0   \n",
      "118       0  ...   0   0   0     0      0       0    0          0      0   \n",
      "119       0  ...   0   0   0     0      0       0    0          0      0   \n",
      "\n",
      "     yuletide  \n",
      "0           0  \n",
      "1           0  \n",
      "2           0  \n",
      "3           0  \n",
      "4           0  \n",
      "..        ...  \n",
      "115         0  \n",
      "116         0  \n",
      "117         0  \n",
      "118         0  \n",
      "119         0  \n",
      "\n",
      "[120 rows x 1569 columns]\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create a new dataframe containing the BOW outputs and the corresponding words as columns. And print it\n",
    "tokens = countVectorizer.get_feature_names_out()\n",
    "print(tokens)\n",
    "\n",
    "lyricsDF = pd.DataFrame(BOW, columns=tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well as you see we're still having some issue, we have some tokens that are not words, like '10' or '2000'.\n",
    "\n",
    "To get rid of that, we could use directly regular expressions within the function. Another solution would be to make preprocessing before using the function *CountVectorizer*.\n",
    "\n",
    "For the moment, we won't pay attention to this issue. But if you are curious and have time, you can find on google how to remove those words using the *CountVectorizer*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to see what are the most used words by Coldplay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oh'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum_bow = lyricsDF.sum()\n",
    "sum_bow.idxmax()\n",
    "\n",
    "# I am not surprised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is the most used word? Are you surprised?\n",
    "\n",
    "Now make a sort in order to show the 10 most used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oh      334\n",
      "don     190\n",
      "know    137\n",
      "just    136\n",
      "ll      132\n",
      "come    126\n",
      "yeah    111\n",
      "love     95\n",
      "ooh      95\n",
      "want     86\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# TODO: print the 10 most used word by Coldplay\n",
    "sumBowSort = sum_bow.sort_values(ascending=False)\n",
    "print(sumBowSort[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here it is! You know the Coldplay lyrics more than the singers now!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
